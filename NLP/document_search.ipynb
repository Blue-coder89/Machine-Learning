{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords,twitter_samples\n",
    "from nltk.stem import PorterStemmer\n",
    "import gensim\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data\n",
    "\n",
    "Download\n",
    "* English embeddings from Google code archive word2vec\n",
    "[look for GoogleNews-vectors-negative300.bin.gz](https://code.google.com/archive/p/word2vec/)\n",
    "    * You'll need to unzip the file first.\n",
    "* and the French embeddings from\n",
    "[cross_lingual_text_classification](https://github.com/vjstark/crosslingual_text_classification).\n",
    "    * in the terminal, type (in one line)\n",
    "    `curl -o ./wiki.multi.fr.vec https://dl.fbaipublicfiles.com/arrival/vectors/wiki.multi.fr.vec`\n",
    "\n",
    "The two files will be named as \n",
    "* `GoogleNews-vectors-negative300.bin`\n",
    "* `wiki.multi.fr.vec`\n",
    "\n",
    "These files have been used in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_embeddings = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin',binary=True)\n",
    "fr_embeddings = KeyedVectors.load_word2vec_format('./wiki.multi.fr.vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "all_tweets = all_positive_tweets + all_negative_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    '''\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        tweets_clean: a list of words containing the processed tweet\n",
    "\n",
    "    '''\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "            word not in string.punctuation):  # remove punctuation\n",
    "            # tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_embedding(document:str,en_embeddings):\n",
    "    \"\"\"\n",
    "    Calculates the embedding vector for a given document.\n",
    "    \n",
    "    Args:\n",
    "        document (str): The input document.\n",
    "        en_embeddings: The word embeddings model.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: The document embedding vector.\n",
    "    \"\"\"\n",
    "    \n",
    "    processed_doc = process_tweet(document)\n",
    "    document_embedding = np.zeros(300)\n",
    "    for i in range(len(processed_doc)):\n",
    "        try:\n",
    "            document_embedding += en_embeddings.get_vector(processed_doc[i])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return document_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_vecs(all_docs, en_embeddings):\n",
    "    '''\n",
    "    Input:\n",
    "        - all_docs: list of strings - all tweets in our dataset.\n",
    "        - en_embeddings: dictionary with words as the keys and their embeddings as the values.\n",
    "    Output:\n",
    "        - document_vec_matrix: matrix of tweet embeddings.\n",
    "        - ind2Doc_dict: dictionary with indices of tweets in vecs as keys and their embeddings as the values.\n",
    "    '''\n",
    "    ind2Doc_dict = {}\n",
    "    document_matrix = []\n",
    "    for index,doc in enumerate(all_docs):\n",
    "        document_embedding = get_document_embedding(doc,en_embeddings)\n",
    "        ind2Doc_dict[index] = document_embedding\n",
    "        document_matrix.append(document_embedding)\n",
    "    document_matrix = np.vstack(document_matrix)\n",
    "    return document_matrix,ind2Doc_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings of each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vecs, ind2Tweet = get_document_vecs(all_tweets, en_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PLANES = 10\n",
    "N_UNIVERSES = 25\n",
    "N_DIMS = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing planes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "def generate_planes(N_DIMS, N_PLANES, N_UNIVERSES):\n",
    "    \"\"\"\n",
    "    Generate random planes for document search.\n",
    "\n",
    "    Parameters:\n",
    "    - N_DIMS (int): Number of dimensions for each plane.\n",
    "    - N_PLANES (int): Number of planes to generate.\n",
    "    - N_UNIVERSES (int): Number of universes.\n",
    "\n",
    "    Returns:\n",
    "    - planes_l (list): List of randomly generated planes.\n",
    "    \"\"\"\n",
    "    planes_l = [np.random.normal(size=(N_DIMS,N_PLANES)) for _ in range(N_UNIVERSES)]\n",
    "    return planes_l\n",
    "\n",
    "planes_l = generate_planes(N_DIMS, N_PLANES, N_UNIVERSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_value_of_vector(v, planes):\n",
    "    \"\"\"Create a hash for a vector; hash_id says which random hash to use.\n",
    "    Input:\n",
    "        - v:  vector of tweet. It's dimension is (1, N_DIMS)\n",
    "        - planes: matrix of dimension (N_DIMS, N_PLANES) - the set of planes that divide up the region\n",
    "    Output:\n",
    "        - res: a number which is used as a hash for your vector\n",
    "    \"\"\"\n",
    "    h = 0\n",
    "    for i in range(N_PLANES):\n",
    "        p = planes[:,i]\n",
    "        h += (np.sign(np.dot(p,v.T))>=0)*np.power(2,i)\n",
    "    return h.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hash_table(vecs, planes):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - vecs: list of vectors to be hashed.\n",
    "        - planes: the matrix of planes in a single \"universe\", with shape (embedding dimensions, number of planes).\n",
    "    Output:\n",
    "        - hash_table: dictionary - keys are hashes, values are lists of vectors (hash buckets)\n",
    "        - id_table: dictionary - keys are hashes, values are list of vectors id's\n",
    "                            (it's used to know which tweet corresponds to the hashed vector)\n",
    "    \"\"\"\n",
    "    buckets = 2**N_PLANES\n",
    "    hash_table = {i:[] for i in range(buckets)}\n",
    "    id_table = {i:[] for i in range(buckets)}\n",
    "    for i,v in enumerate(vecs):\n",
    "        h = hash_value_of_vector(v,planes)\n",
    "        hash_table[h].append(v)\n",
    "        id_table[h].append(i)\n",
    "    return hash_table,id_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on hash universe #: 0\n",
      "working on hash universe #: 1\n",
      "working on hash universe #: 2\n",
      "working on hash universe #: 3\n",
      "working on hash universe #: 4\n",
      "working on hash universe #: 5\n",
      "working on hash universe #: 6\n",
      "working on hash universe #: 7\n",
      "working on hash universe #: 8\n",
      "working on hash universe #: 9\n",
      "working on hash universe #: 10\n",
      "working on hash universe #: 11\n",
      "working on hash universe #: 12\n",
      "working on hash universe #: 13\n",
      "working on hash universe #: 14\n",
      "working on hash universe #: 15\n",
      "working on hash universe #: 16\n",
      "working on hash universe #: 17\n",
      "working on hash universe #: 18\n",
      "working on hash universe #: 19\n",
      "working on hash universe #: 20\n",
      "working on hash universe #: 21\n",
      "working on hash universe #: 22\n",
      "working on hash universe #: 23\n",
      "working on hash universe #: 24\n"
     ]
    }
   ],
   "source": [
    "### Creating the hashtables\n",
    "hash_tables = []\n",
    "id_tables = []\n",
    "for universe_id in range(N_UNIVERSES):  # there are 25 hashes\n",
    "    print('working on hash universe #:', universe_id)\n",
    "    planes = planes_l[universe_id]\n",
    "    hash_table, id_table = make_hash_table(document_vecs, planes)\n",
    "    hash_tables.append(hash_table)\n",
    "    id_tables.append(id_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_nearest_neighbours(v, candidates: list, k=1):\n",
    "    \"\"\"\n",
    "    Finds the k nearest neighbours to a given vector v from a list of candidate vectors.\n",
    "\n",
    "    Parameters:\n",
    "    v (array-like): The vector for which nearest neighbours need to be found.\n",
    "    candidates (list): A list of candidate vectors.\n",
    "    k (int): The number of nearest neighbours to be returned. Default is 1.\n",
    "\n",
    "    Returns:\n",
    "    list: The indices of the k nearest neighbours in the candidates list.\n",
    "    \"\"\"\n",
    "    similarity_score = []\n",
    "    for c in candidates:\n",
    "        similarity_score.append(np.dot(v, c) / (np.linalg.norm(v) * np.linalg.norm(c)))\n",
    "    sorted_ids = np.argsort(similarity_score)\n",
    "    return sorted_ids[-k:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_knn(doc_id, v, planes_l, k=1, num_universes_to_use=N_UNIVERSES):\n",
    "    \"\"\"Search for k-NN using hashes.\"\"\"\n",
    "    neighbours_to_consider = []\n",
    "    neighbours_to_consider_id = []\n",
    "    neighbours_to_consider_id_set = set()\n",
    "    for i in range(num_universes_to_use):\n",
    "        planes = planes_l[i]\n",
    "        bucket_id = hash_value_of_vector(v,planes)\n",
    "        neighbours = hash_tables[i][bucket_id]\n",
    "        neighbours_id = id_tables[i][bucket_id]\n",
    "        for index,i_d in enumerate(neighbours_id):\n",
    "            if i_d == doc_id: continue\n",
    "            if i_d not in neighbours_to_consider_id_set:\n",
    "                neighbours_to_consider_id_set.add(i_d)\n",
    "                neighbours_to_consider_id.append(i_d)\n",
    "                neighbours_to_consider.append(neighbours[index])\n",
    "    print(\"Fast considering %d vecs\" % len(neighbours_to_consider))\n",
    "    nearest_neighbor_id = k_nearest_neighbours(v,neighbours_to_consider,k=k)\n",
    "    print(nearest_neighbor_id)\n",
    "    print(neighbours_to_consider_id)\n",
    "    return [neighbours_to_consider_id[idx] for idx in nearest_neighbor_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id = 0\n",
    "doc_to_search = all_tweets[doc_id]\n",
    "vec_to_search = document_vecs[doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast considering 939 vecs\n",
      "[16  7 35]\n",
      "[3, 5, 7, 26, 28, 36, 44, 51, 66, 68, 71, 76, 79, 83, 91, 97, 105, 112, 117, 125, 126, 131, 135, 146, 152, 154, 156, 168, 184, 195, 210, 214, 220, 232, 233, 253, 254, 277, 285, 286, 292, 299, 319, 332, 350, 371, 373, 375, 404, 427, 430, 432, 466, 467, 469, 476, 478, 479, 491, 511, 521, 531, 538, 539, 563, 579, 591, 594, 615, 618, 619, 642, 647, 661, 671, 673, 674, 675, 681, 701, 705, 724, 727, 738, 743, 757, 762, 767, 770, 773, 780, 794, 810, 822, 824, 826, 833, 835, 842, 847, 850, 855, 859, 874, 884, 887, 898, 909, 920, 930, 938, 943, 958, 959, 962, 993, 995, 1005, 1012, 1023, 1033, 1039, 1040, 1058, 1065, 1069, 1073, 1075, 1081, 1088, 1107, 1113, 1117, 1142, 1147, 1154, 1172, 1176, 1180, 1185, 1189, 1202, 1211, 1212, 1219, 1220, 1228, 1248, 1249, 1270, 1278, 1280, 1287, 1296, 1304, 1309, 1324, 1328, 1352, 1358, 1383, 1386, 1411, 1415, 1419, 1420, 1423, 1427, 1444, 1451, 1454, 1461, 1462, 1470, 1473, 1477, 1484, 1485, 1488, 1490, 1512, 1513, 1520, 1529, 1543, 1544, 1546, 1553, 1581, 1588, 1601, 1607, 1618, 1621, 1625, 1627, 1631, 1632, 1634, 1650, 1662, 1670, 1674, 1690, 1698, 1701, 1703, 1716, 1721, 1729, 1731, 1732, 1743, 1760, 1762, 1764, 1777, 1794, 1809, 1830, 1836, 1863, 1864, 1866, 1869, 1876, 1887, 1898, 1916, 1917, 1918, 1924, 1935, 1939, 1945, 1953, 1957, 1977, 2002, 2003, 2008, 2012, 2021, 2028, 2032, 2047, 2051, 2057, 2061, 2074, 2077, 2080, 2086, 2106, 2143, 2154, 2159, 2164, 2185, 2201, 2206, 2211, 2227, 2228, 2236, 2246, 2251, 2304, 2311, 2328, 2333, 2359, 2368, 2384, 2385, 2390, 2402, 2416, 2417, 2419, 2427, 2429, 2438, 2439, 2445, 2453, 2454, 2460, 2465, 2468, 2470, 2472, 2478, 2487, 2498, 2503, 2509, 2510, 2512, 2516, 2528, 2537, 2552, 2554, 2584, 2595, 2634, 2638, 2651, 2654, 2688, 2692, 2693, 2699, 2700, 2706, 2708, 2728, 2736, 2738, 2744, 2747, 2763, 2768, 2780, 2782, 2786, 2812, 2813, 2832, 2837, 2865, 2889, 2898, 2902, 2923, 2928, 2937, 2939, 2942, 2957, 2963, 2967, 2984, 3014, 3026, 3042, 3048, 3051, 3052, 3054, 3072, 3078, 3080, 3088, 3091, 3100, 3130, 3149, 3153, 3159, 3180, 3206, 3210, 3213, 3217, 3223, 3231, 3234, 3237, 3238, 3260, 3264, 3272, 3289, 3290, 3301, 3330, 3356, 3369, 3379, 3382, 3399, 3409, 3432, 3436, 3446, 3456, 3465, 3466, 3467, 3480, 3488, 3490, 3517, 3525, 3531, 3539, 3579, 3581, 3583, 3584, 3587, 3602, 3629, 3634, 3636, 3639, 3640, 3698, 3706, 3708, 3715, 3723, 3737, 3740, 3742, 3748, 3771, 3773, 3779, 3786, 3793, 3794, 3798, 3812, 3815, 3818, 3831, 3837, 3843, 3847, 3874, 3881, 3902, 3903, 3906, 3921, 3930, 3934, 3935, 3947, 3954, 3957, 3965, 3975, 3977, 3999, 4007, 4013, 4025, 4046, 4053, 4094, 4095, 4101, 4104, 4106, 4111, 4116, 4141, 4142, 4178, 4185, 4209, 4214, 4216, 4240, 4257, 4258, 4273, 4276, 4280, 4293, 4318, 4320, 4326, 4337, 4339, 4351, 4362, 4370, 4371, 4380, 4395, 4397, 4399, 4404, 4406, 4407, 4423, 4448, 4453, 4464, 4466, 4479, 4493, 4506, 4543, 4544, 4550, 4556, 4579, 4592, 4598, 4599, 4605, 4624, 4625, 4629, 4646, 4651, 4657, 4660, 4665, 4682, 4701, 4711, 4721, 4739, 4742, 4746, 4750, 4754, 4774, 4783, 4792, 4798, 4806, 4807, 4830, 4833, 4841, 4852, 4854, 4856, 4874, 4877, 4883, 4885, 4899, 4903, 4917, 4931, 4933, 4937, 4945, 4949, 4953, 4969, 4981, 4985, 4987, 4988, 4995, 5015, 5030, 5077, 5107, 5131, 5195, 5223, 5257, 5309, 5327, 5415, 5463, 5583, 5648, 5702, 5746, 5778, 5792, 5800, 5801, 5804, 5821, 5888, 5969, 6023, 6030, 6053, 6054, 6074, 6199, 6233, 6263, 6340, 6351, 6355, 6391, 6394, 6497, 6559, 6583, 6586, 6625, 6643, 6663, 6685, 6727, 6737, 6818, 6832, 6875, 6893, 6968, 7019, 7022, 7029, 7032, 7118, 7137, 7147, 7156, 7181, 7223, 7228, 7238, 7261, 7281, 7312, 7335, 7339, 7441, 7504, 7506, 7636, 7704, 7727, 7765, 7804, 7828, 7833, 7899, 7938, 8011, 8066, 8109, 8135, 8144, 8146, 8161, 8179, 8220, 8228, 8242, 8272, 8431, 8469, 8494, 8591, 8620, 8788, 8825, 8858, 8859, 8864, 8879, 8885, 8908, 8916, 8968, 9029, 9099, 9102, 9177, 9247, 9289, 9348, 9386, 9439, 9449, 9451, 9524, 9612, 9688, 9760, 9769, 9790, 9984, 9990, 19, 166, 416, 524, 1068, 1158, 1253, 1393, 1494, 1582, 2212, 2508, 2896, 2976, 3884, 3933, 4016, 4223, 4245, 4446, 4690, 4880, 5024, 5039, 5160, 5414, 5619, 5699, 5725, 5797, 5846, 6009, 6271, 6389, 6408, 6642, 6793, 6799, 6974, 7296, 7301, 7311, 7408, 7606, 7646, 7696, 7710, 7772, 7782, 7959, 8023, 8101, 8302, 8349, 8446, 8582, 8609, 8699, 8988, 9019, 9128, 9130, 9131, 9250, 9483, 9740, 9761, 9832, 449, 5233, 6426, 7749, 8283, 20, 77, 127, 172, 219, 2360, 3923, 4492, 5218, 5498, 5848, 6594, 8497, 8892, 9913, 22, 24, 46, 107, 115, 143, 200, 216, 243, 246, 250, 261, 291, 297, 308, 315, 333, 364, 414, 425, 458, 470, 480, 481, 501, 544, 549, 556, 560, 585, 633, 685, 691, 694, 719, 819, 895, 929, 931, 1004, 1035, 1161, 1232, 1247, 1265, 1269, 1284, 1318, 1329, 1528, 1572, 1702, 1725, 1806, 1813, 1978, 1988, 1993, 2041, 2197, 2273, 2293, 2308, 2324, 2421, 2463, 2578, 2620, 2659, 2667, 2697, 2698, 2828, 2911, 3033, 3040, 3103, 3111, 3137, 3190, 3212, 3300, 3316, 3329, 3335, 3386, 3410, 3501, 3699, 3703, 3712, 3717, 3726, 3768, 3808, 3814, 3821, 3890, 3916, 3927, 3938, 3939, 3951, 3961, 3970, 3978, 3989, 3994, 4004, 4018, 4019, 4021, 4032, 4039, 4051, 4086, 4138, 4169, 4189, 4201, 4235, 4263, 4275, 4277, 4357, 4358, 4378, 4400, 4451, 4522, 4529, 4572, 4643, 4645, 4704, 4735, 4763, 4835, 4871, 4895, 4952, 4972, 5198, 5365, 5431, 5657, 5899, 6033, 6177, 6380, 6500, 6629, 6842, 6855, 7094, 7188, 7258, 7515, 7540, 7667, 7848, 8932, 9104, 9133, 9427, 9509, 9516, 9588, 9817]\n"
     ]
    }
   ],
   "source": [
    "nearest_neighbor_ids = approximate_knn(doc_id, vec_to_search, planes_l, k=3, num_universes_to_use=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors for document 0\n",
      "Document contents: #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "\n",
      "Nearest neighbor at document id 105\n",
      "document contents: #FollowFriday @straz_das @DCarsonCPA @GH813600 for being top engaged members in my community this week :)\n",
      "Nearest neighbor at document id 51\n",
      "document contents: #FollowFriday @France_Espana @reglisse_menthe @CCI_inter for being top engaged members in my community this week :)\n",
      "Nearest neighbor at document id 253\n",
      "document contents: #FollowFriday @CCIFCcanada @AdamEvnmnt @boxcalf1 for being top engaged members in my community this week :)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nearest neighbors for document {doc_id}\")\n",
    "print(f\"Document contents: {doc_to_search}\")\n",
    "print(\"\")\n",
    "\n",
    "for neighbor_id in nearest_neighbor_ids:\n",
    "    print(f\"Nearest neighbor at document id {neighbor_id}\")\n",
    "    print(f\"document contents: {all_tweets[neighbor_id]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
